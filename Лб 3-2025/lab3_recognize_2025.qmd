---
title: "_Лабораторна робота № 2_. Розвідувальний аналіз даних"
description:
  Документ зроблено в [Quarto](https://quarto.org/)
author: "&copy; [Valeriy Sydorenko](https://www.linkedin.com/in/valeriy-sydorenko-6782279a/), 2025"
date: "`r Sys.Date()`"
lang: ukr
format:
  html:
    # theme: quartz
    code-fold: true
    toc: true # меню
    toc_float: # спливаюче меню  
      collapsed: true # авто
      number_sections: true
      # css: styles.css
editor: visual
bibliography: references_lab.bib
---

**Мета:** *Засвоєння принципів, знайомство з інструментами та набуття навичок експлораторного (розвідувального) аналізу даних засобами мови програмування R та колекції пакетів `dplyr`, `ggplot2`, `desctable`, `GGally`, `corrplot`, `PerformanceAnalytics`, `FactoMineR`, `factoextra`, `desctable`, `ade4`, `psych`, `smacof`, `WVPlots`, `caret`, `car`.*

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Інсталюємо необхідні пакети
packageNeed <- c("knitr", "dplyr", "ggplot2", "devtools", "GGally", "corrplot", "PerformanceAnalytics", "FactoMineR",
                 "factoextra", "desctable", "ade4", "psych",
                 "smacof", "WVPlots", "caret", "car")
# install.packages(packageNeed)

# Встановлення та завантаження пакету devtools, якщо він ще не встановлений
# if (!require(devtools)) {
#   install.packages("devtools")
#   library(devtools)
# }
# 
# # Інсталювати пакет funModeling з GitHub
# devtools::install_github("pablo14/funModeling")
# library(funModeling)



lapply(packageNeed, library, character.only = TRUE)
```

## Що ви будете вміти?

-   Розв'язувати задачі розвідувального аналізу даних засобами мови R у середовищі IDE RStudio.
-   Візуалізувати результати аналізу.
-   Створювати звіт за допомогою видавничої системи Quarto

::: callout-tip
## Примітка.

Див. [Електронний підручник](https://vgamaley.github.io/DS-book-lab/lab_3.html)
:::

## Короткі теоретичні відомості

### Що таке розвідувальний аналіз даних?

У рамках життєвого циклу процесу Data Mining згідно з методологією CRISP DM [@CRISP_DM], першою фазою аналізу даних є "Розуміння даних" (**Understanding**) (рис. 1). До цієї фази входять розглянуті у попередніх лабораторних роботах задачі збору та різного роду трасформації. Наступною важливою задачею даної фази є вивчення даних, що складає основу так званого [*розвідувального аналізу даних*](https://en.wikipedia.org/wiki/Exploratory_data_analysis) (Exploratory data analysis, EDA).

**Розвідувальний аналіз даних** -- аналіз основних властивостей даних, виявлення в них загальних закономірностей, розподілів та аномалій, побудова початкових моделей шляхом їх перетворення та/або представлення у зручному вигляді: графічному, табличному, схем, діаграм і т.ін. [@EDA]

![*Рис. 1. Структура задачі розуміння даних у складі Data Science-проекту* [@r4ds]](image/wrangling.png)

Термін EDA був введений математиком Джоном Тьюкі, який сформулював цілі РАД таким чином:

-   максимальне "проникнення" в дані
-   виявлення основних структур
-   вибір найважливіших змінних
-   виявлення відхилень та аномалій
-   перевірка основних гіпотез
-   розробка початкових моделей

До основних інструментів РАД відносятся:

-   аналіз ймовірностних розподілів змінних
-   побудова та аналіз кореляційних матриць
-   методи зниження розмірності даних: факторний аналіз, дискримінантний аналіз, багатовимірне шкалювання та ін.

РАД -- це ітеративна процедура, в результаті якої ми:

-   формулюємо запитання щодо наших даних
-   шукаємо відповідь за допомогою візуалізації, трасформації та моделювання наявних даних
-   аналізуємо те, що отримано в результаті аналізу та формулюємо нові запитання.

"РАД є важливою частиною будь-якого аналізу даних, навіть якщо питання надається вам на блюді, тому що вам завжди потрібно вивчити якість ваших даних. Очищення даних -- це лише одне застосування РАД: ви задаєте питання про те, чи відповідають ваші дані вашим очікуванням чи ні." [@r4ds].\
Для очищення даних нам потрібно буде розгорнути всі інструменти РАД: візуалізацію, перетворення та моделювання.

### Питання

Не існує правил, яким ми могли би керуватися, в частині того, які питання нам слід задавати в процесі своїх досліджень. Проте два типи питань завжди будуть корисними для того, щоб робити відкриття у наших даних. У самій довільній формі ці питання можна сформулювати так [@r4ds]:

-   Який тип варіації має місце в моїх змінних?
-   Який тип коваріації має місце між моїми змінними?

### Типи шкал вимірювань

У попередніх темах нами детально були розглянуті шкали вимірювань, математичні операції, які мають сенс з результатми вимірювання у відповідній шкалі та відповідні статистичні характеристики, які мають сенс. Нагадаємо коротко про них:

-   **Шкала найменувань (номінальна шкала)**, є найпростішою формою шкали, де єдині можливі відносини між елементами -- це відносини рівності та нерівності. Ця шкала використовується для класифікації об'єктів на основі спільних характеристик і не передбачає встановлення порядку або відношень "більше-менше" між елементами. Вона застосовується для опису *якісних* характеристик (експериментальні групи, стать, політичні партії, країни та ідентифікаційні номери, наприклад, номери на майках футболістів).

-   **Порядкова шкала (рангова шкала)** -- це також шкала більш високого рангу, де можливі відношення "більше - менше", так я к дозволяє ранжувати результати вимірювань у порядку зростання, чи убування певної якості. Наприклад, шкала "Задовільно", "Добре" і "Відмінно" відповідає ранговій шкалі. "Відмінно" більше "добре", "ДОбре" більше "Задовільно", і відповідно "Відмінно" більше "Задовільно". Однак ми не можемо стверджувати, що різниця між "Відмінно" і "Добре" дорівнює різниці між "Добре" і "Задовільно". Тому, навіть якщо ми призначимо числові значення, наприклад, "Відмінно" - 5, "Добре" - 3 і "Задовільно" - 2, ми не можемо здійснити арифметичні операції з цією шкалою, такі як обчислення середнього значення.

-   **Інтервальна шкала (шкала різниць)** -- дозволяє порівнювати різниці між інтервалами значень. Наприклад, різниця між 10 градусами Цельсія і 20 градусами Цельсія дорівнює різниці між 80 градусами Цельсія і 90 градусами Цельсія. Для цієї шкали можна виконувати операції порівняння середніх значень, але операції множення та ділення не мають сенсу, оскільки нуль на шкалі різниць є відносним. Наприклад, ми не можемо стверджувати, що 20 градусів Цельсія вдвічі тепліше, ніж 10 градусів Цельсія, оскільки нуль градусів Цельсія -- це лише умовна точка, яка відповідає температурі плавлення льоду.

-   Шкала відношень, відома також як абсолютна шкала, визначається як "найповноцінніша" шкала, оскільки вона має природний і однозначний початок координат. Наприклад, маса в кілограмах або температура в градусах Кельвіна, а не Цельсія, є прикладами шкал відношень. На цих шкалах можна виконувати всі арифметичні операції, такі як додавання, віднімання, множення та ділення, оскільки нуль на цих шкалах має абсолютне значення. Тобто, на шкалі відношень можна порівнювати відношення значень і робити обчислення, що є неможливим на інших типах шкал.

найбльш загальні питання розвідувального аналізу даних, зазначені вище щодо типу варіанції і ковариації залишаються актуальними для вимірів у буль-якій шкалі.

### Варіація

**Варіація (Variation)** -- це тенденція до змін значення змінної від вимірювання до вимірювання. Ми можете легко помітити варіації в реальному житті; якщо ми двічі вимірюємо постійну змінну, ми отримаємо два різні результати.\
Категоричні змінні також можуть відрізнятися, якщо виміри робити для різних суб'єктів (наприклад, кольори очей різних людей) або у різні моменти часу (наприклад, енергетичні рівні електрона в різні моменти). Кожна змінна має свій власний паттерн у варіації, який може виявити цікаву інформацію. Найкращий спосіб зрозуміти цю закономірність -- візуалізувати розподіл значень змінної.

Візьмемо для дослідження відомий датасет `iris` і на його прикладі проведемо розвідувальний аналіз даних, водночас продемонстувавши можливі варіанти відповідного інструментарію.

#### Візуалізація розподілу

Спочатку завжди доцільно продивитися таблицю датасета, вивести основні статистичні характеристики та побудувати розподіли досліджуваних змінних.

```{r}
DT::datatable(iris)
```

Нижче наведено приклад розподілу категоріальної змінної `Species`.

```{r echo=TRUE, message=FALSE, paged.print=FALSE}
library(tidyverse)
library(ggplot2)
iris |> 
  ggplot() +
  geom_bar(mapping = aes(x = Species))
```

Частоту для кожного значення категоріальної змінної можна обчислита, наприклад, так:

```{r}
iris |>  
  count(Species)
```

Для неперервних змінних доцільно побудувати гістограму.

```{r}
iris |> 
  ggplot() +
  geom_histogram(mapping = aes(x = Petal.Length), binwidth = 0.5)
```

Інтервальна таблиця частот, що відповідає гістограмі, може бути обчислена так:

```{r}
iris |> 
  count(cut_width(Petal.Length, 0.5)) 
```

Можна побудувати гістограму для певної долі значень:

```{r}
smaller <- iris |> 
  filter(Petal.Length > 2.25)
  
smaller |> 
  ggplot(mapping = aes(x = Petal.Length)) +
  geom_histogram(binwidth = 0.1)
```

Часто буває доцільно поудвати серія гістограм для різних груп спостережень:

```{r}
iris |> 
  ggplot(mapping = aes(x = Petal.Length, colour = Species)) +
  geom_freqpoly(binwidth = 0.1)
```

Після того, як ми виконали візуалізацію, що ми маємо знайти на цих графіках? Яка може бути послідовність запитань на наступному етапі?

\*Типові запитання з урахуванням специфіка даної задачі можуть виглядати так:

-   Які значення є найбільш поширеними? Чому?
-   Які значення є рідкісними? Чому? Це відповідає нашим очікуванням?
-   Чи бачемо ми якісь незвичайні закономірності? Що може їх пояснити?

#### Незвичайні значення

Як правило у вибіркових даних зустірчаються викиди (outliers) -- такі значення свідчать або про похибку вимірювання, або про якість надзвичайні причини, що потребують уважного вивчення.

```{r}
iris |> 
  filter(Species == "virginica") |> 
  ggplot() + 
  geom_histogram(mapping = aes(x = Petal.Length), binwidth = 0.5)
```

Якщо приймається рішення їх видалити, то це можна зроити наступним чином:

```{r}
unusual <- iris |> 
  filter(Petal.Length < 3 | Petal.Length > 6) |> 
  select(-Species) |>
  arrange(Petal.Length)
unusual
```

#### Пропущені значення (Missing values)

Часто на практиці дані виявляються некомплектними -- мають місце пропущенні дані (`NA`). У таких випадках відомі два виходи з систуації:

-   видалити некомплектні спостереження
-   виконати імпутацію пропущених значень -- замінити пропущені значення певними у відповідності з якимось алгоритмом.

Пакет `ggplot2` автоматично видаляє некмоплектні дані:

```{r}
iris
iris2 <- iris |> 
  mutate(y = ifelse(Petal.Length < 3 | Petal.Length > 20, NA, Petal.Length))
```

```{r}
ggplot(data = iris2, mapping = aes(x = Petal.Length, y = Petal.Width)) + 
  geom_point()
```

### Коваріація

Якщо варіація описує поведінку в межах змінної, коваріація описує поведінку між змінними.\
**Коваріація (Covariation)** -- це схильність значень двох чи більше змінних змінюватися разом. Найкращим способом виявлення коваріації є візуалізація відносин між двома чи більше змінних.

##### Категоріальні та неперервні змінні

Природнім є рішення щодо вивчення розподілу неперервної змінної, розбивши її на групи у відповідності до значень категоріальної змінної.

Аналіз щільностей розподілів у цьому випадку є не зовсім зручним. Альтернативним варіантом представлення аналогічної інформації є п'ятиквантильний графік (boxplot, box and wiskers plot), відомий як "боксплот", або "ящик з вусами". Боксплот акумулює в собі всі найважливіші інтегральні харакетристики стосовно мір центральної тенденції, розсіювання та форми розподілу (рис. 3).

![*Рис. 3. Структура боксплота* [@r4ds]](image/boxplot.png)

Тоді для нашого випадку застосування боксплотів дасть такий результат:

```{r}
ggplot(data = iris, mapping = aes(x = Species, y = Petal.Length)) +
  geom_boxplot()
```

#### Дві категоріальні змінні

Для візуалізації коваріації між категоріальними змінними необхідно візуалізувати частоти: у вигляді таблиці, або певного графічного візуалізатора. Наприклад:

```{r}
ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color))
```

З результатів видно, що існує певна залежність між кольором діаманта, та якістю його ограненості.

### Зниження розмірності даних

[**Зниження розмірності (Dimensionality reduction)**](https://en.wikipedia.org/wiki/Dimensionality_reduction) -- процесс скорочення кількості випадкових змінних шляхом отримання гооловних змінних. Цей процес можно поділити *обирання ознак* та *виділяння ознак*.\
*Обирання ознак* -- це процес пошуку первісних змінних (факторів), що починаються в рамках фази розуміння даних із залученням експертів предметної галузі і з залученням всього арсеналу інстурментів маніпулювання даними, про що йшлося вище. (Маніпулятивні методики).\
*Проектування ознак* -- це перетворення даних з багатовимірного простору у простір простір невеликої кількості вимірів. (Математичні методики). (Далі під зниженням розмірнонсті будем мати на увазі саме проектування ознак). Існує велика кількість лінійних і нелінійних методик зниження розмірності.\
Що дає зниження розмірності на практиці? В першу чергу спрощення представлення багатовимірних даних, їх візуалізацію, вирішення задач класифікації та регресії і, власне, краще розуміння процесів, що моделюються.\
Одним з фундаментальних лінійних методів зниження розмірності, що широко викорстовується на практиці, є [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) [@PCA].

#### Постановка задачі аналізу методом главних компонент (PCA)

Припустимо [@PCA], що ми маємо випадковий вектор $X$: $$
X=\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
$$

З коваріаційною матрицею:\
$$
var(X) = \Sigma = 
\begin{pmatrix}
\sigma_{1}^2 & \sigma_{12} & \ldots & \sigma_{1p}\\
\sigma_{21} & \sigma_{2}^2 & \ldots & \sigma_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
\sigma_{p1} & \sigma_{p2} & \ldots & \sigma_{p}^2
\end{pmatrix}
$$

Мета [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) [@PCA_b] полягає в пошуку $k$ лінійних комбінацій $p$ змінних $X$, що містять найбільшу дисперсію. Лінійна комбінація має насупний вигляд:

$Y_1 = a_{11} X_1 + a_{12} X_2 + \cdots + a_{1p} X_p$

$Y_2 = a_{21} X_1 + a_{22} X_2 + \cdots + a_{2p} X_p$

$\vdots$

$Y_k = a_{k1} X_1 + a_{k2} X_2 + \cdots + a_{kp} X_p$

При цьому $\sum\limits_{i=1}^pa_{1i}^2=1$ і т. д.

Дисперсия першої главної компоненти $var(Y_1)=a_1'\Sigma a_1$, де $\Sigma$ -- ковариаційна матриця.

Аналогічно обчислюється дисперсія другої і т. д. головних компонент.

У даній моделі вектори $a_i'=(a_{i1}, a_{i2},...,a_{ip})',\;i=\overline{1,p}$ представляють *власні вектори* ковариаційної матриці $\Sigma$, тоді як дисперсія $i$-ої головної компоненти дорівнює *власному значенню* матриці ковариацій:

$var(Y_i)=\lambda_i$.

*Загальна дисперсія* вибірки дорівнює $\sum\limits_{i=1}^p\lambda_{i}.$\
Метод головних компонент (PCA) вирішуючи задачу зниження розмірності дозволяє одночасно вирішити задачу сегементації (кластеризації) -- тобто з'ясувати, чи є досліджувані дані однорідними, чи сегментовані на групи зі схожими ознаками. Відповідь на це питання є однією з головних задач експлораторного аналізу, що передує етапу побуви більш складних моделей класифікації, регресії чи моделей на основі асоціативних правил.

В арсеналі R існує багато інструментів для реалізації зниження розмірності, зокрема PCA.

## Приклад виконання індивідуального завдання

### Постановка задачі

Дано стандартний багатовимірний набір даних [Іриси Фішера (англ. Iris flower data set)](https://en.wikipedia.org/wiki/Iris_flower_data_set).\
Іриси Фішера складаються з даних про 150 вимірювань ірисів з трьох видів ---- Iris setosa, Iris virginica і Iris versicolor, по 50 вимірювань на вид. Для кожного екземпляра вимірювалися чотири характеристики (в сантиметрах):

-   Довжина зовнішньої частки оцвітини (англ. sepal length);
-   Ширина зовнішньої частки оцвітини (англ. sepal width);
-   Довжина внутрішньої частки оцвітини (англ. petal length);
-   Ширина внутрішньої частки оцвітини (англ. petal width).

Для даного набору виконати розвідувальний аналіз даних.\
Необхідно дати відповідь на такі питання:

-   чи є дані комплектними? з'ясувати характер розподілу змінних.
-   чи пов'язані між собою характеристики? якщо пов'язані, то які і як?
-   чи існує проста структура, яка дозволяє описати дані об'єкти в просторі розмірністю менш ніж `4`? якщо існує -- побудувати таку модель.
-   чи є всі три види ірисів однорідними у даному просторі ознак? якщо ні, то виконати сегментацію -- знайти групи з однорідними ознаками.\
    Результати представити із застосуванням простих і зрозумілих візуалізаторів.

<!-- Іриси Фішера (англ. Iris flower data set) — це багатовимірний набір даних для задачі класифікації, на прикладі якого англійський статистик та біолог Рональд Фішер в 1936 році продемонстрував роботу розробленого ним методу дискримінантного аналізу. Іноді його також називають ірисами Андерсона — через те, що дані були зібрані американським ботаніком Едгаром Андерсоном. Цей набір даних став класичним і часто використовується в літературі для ілюстрації роботи різних статистичних алгоритмів. -->

### Виконання завдання

Дані з досліджуваного набору мають наступний вигляд:

```{r}
iris |> 
  head()
# iris |>
#   desctable()
```

Обчислимо і дослідимо сумарні статистики.

```{r}
# Сводные выборочные характеристики
iris |> 
  summary() 
```

Що ми бачимо?

-   всі дані мають числову природу
-   дані комплектні: відстуні пропущені значення
-   відсутні нульові значення -- це знімає можливі проблеми при трансформації
-   відсутні надвелики значення -- надвелики значення кожен раз потребують серйозної уваги і аналізу можливих причин, що їх викликали
-   по всіх змінних дані мають варіацію по унікальним значенням одного порядку.

Дослідимо закони розподілу кожної з чотирьох змінних. Для цього побудуємо серію гістограм для кожної з чотирьох числових змінних: `"Sepal.Length"`, `"Sepal.Width"`, `"Petal.Length"`, `"Petal.Width"`

```{r}
# Побудова 4x4 матриці гістограм і щільності нормального розподілу з відносними частками
library(tidyverse)
iris |>
  pivot_longer(cols = -Species) |>
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 0.2, 
                 fill = "lightblue", 
                 color = "dark grey", 
                 aes(y = after_stat(density))) +
  geom_density(aes(y = after_stat(density)), color = "red") +
  facet_wrap(~name, scales = "free") +
  theme_minimal() +
  labs(x = "Значення", y = "Відносна частота")
```

Що ми бачимо?

-   статистичні розподіли змінних `"Sepal.Length"`, `"Sepal.Width"` мають дзвоноподібну форму, наближену до нормального. Враховуючи, що значення оцінок асимметрії та ексцесу несуттєво відрізняються від нуля, в першому наближенні можна вважати дані розподіли нормальними. Про що це говорить і що це дає? По-перше, це говорить про те, що доля малих і великих даних врівноважують одна одну, по-друге, нормальність законів розподілу досліджуваних величин, або, принаймні, "натяк" на нормальність **завжди добре**, тому що класичними передумовами для коректної побудови великої кількості різного роду моделей вимагає від даних нормального закону розподілу, чи, принаймні, симетричності закону розподілу. В нашому випадку це є передумовою однорідного розподілу спостережень у просторі інформативних ознак, що є позитивним моментом при вирішенні задачі сегментації.\
-   статистичні розподіли змінних `"Petal.Length"`, `"Petal.Width"` на відміну від двох інших, мають чітку бімодальну структуру, що гооврить про явно виражену неоднорідність даних і про те, що саме ці дві змінні є дискримінуючими у просторі досліджуваних ознак; це важливо для побудови задачі сегментації

Для відповіді на питання, чи пов'язані між собою змінні, застосуємо кореляційний аналіз. З урахуванням числової природи даних, для оцінки кореляції скористаємося коефіцієнтом кореляції Пірсона. Враховуючі багатомірний аналіз початкових даних, важливо вдало підібрати візуалізатор. Нижче запропоновано два з найбільш відомих і поширених.

```{r}
# Кореляція Пірсона
iris |>
  select(-Species) |>
     cor() |>
  corrplot(order = "hclust", tl.col='black', tl.cex=.75) 
  # chart.Correlation(histogram=TRUE, pch=19)
```

```{r}
# pairs(iris[1:4], main="Edgar Anderson's Iris Data", font.main=4, pch=19)

iris |>
  select(-Species) |> 
  pairs( main="Edgar Anderson's Iris Data", font.main = 4, pch = 19, col = iris$Species)

# pairs(iris[1:4], main="Edgar Anderson's Iris Data", font.main = 4, pch = 19, col = iris$Species)
```

```{r}

df_iris <- iris |> 
  select(-Species)
 # df_iris |>
 #   correlation_table("")
 
df_iris |>
    cor() |>
  # head(11)
  knitr::kable(caption = "Таблица оценок коэффициентов корреляции") 
```

Що ми бачимо?

-   має місце сильний позитивний кореляційний зв'язок між змінною `Sepal.Length` та змінними `Petal.Length`, `Petal.Width`; на кореляційних полях чітко видно наявність даної кореляції
-   на кореляційних полях чітко видно наявність неоднородності даних -- дані чітко поділяються на гомогенні групи за змінною `Species`
-   має місце слабкий від'ємний кореляційний зв'язок `Petal.Length` і `Sepal.Width`; дана кореляція є уявною в силу сегментованості даних по змінній `Species`: якщо уважно дослідити форму кореляційних полів для кожного значення данної змінної, то можна побачити, що всередині кожного сегменту має місце позитивна кореляція

Що це нам дає?

-   наявність високого ступеня кореляції дає можливість знизити розмірність даних і знайти просту структуру у просторі меншої розмірності
-   у просторі меншої розмірності можна можна виконати сегментацію даних.

Для зниження розмірності і одночасно сегментації даних скористаємося методом головних компонент (PCA).

```{r}
# PCA 
resPCA <- iris |> 
  select(-Species) |>
  PCA(ncp = 8, graph = FALSE)
```

```{r}
# власні значення та кумулятивний процент
eigenvalues <- as.data.frame(resPCA$eig)
cumVar <- round(eigenvalues$`cumulative percentage of variance`[length(eigenvalues$eigenvalue[eigenvalues$eigenvalue >= 0.9])], 2)
```

```{r}
knitr::kable(
  eigenvalues, 
  caption = "Власні значення (eigenvalues) і сумарний процент поясненої дисперсії"
)
```

```{r}
# 
fviz_screeplot(resPCA, addlabels = TRUE,  ncp=10)
```

Чщо ми бачимо?\
Ми маємо $p=$ `r length(eigenvalues$eigenvalu[eigenvalues$eigenvalue >= 0.9])` головних компонент, які пояснюють `r cumVar` % дисперсії. Це значить, що м маємо всього дві нові компоненти замість чотирьох і практично без втрати інформації можемо представити всі спостереження в системі двох координат на площині: перша компонента по осі `Х`, друга -- по осі Y (див. рис.).\
Проаналізуємо детально структуру двох перших компонент, виключивши решту незначимих (див. табл. і рис.).

```{r}
# Навантаження для двох перших головних компонент
knitr::kable(
  resPCA$var$coord[ ,1:2], 
  caption = "Таблиця навантажень"
)

```

Що ми бачимо?

-   Структуру першої компоненти головним чином складають три початкові змінні: `Sepal.Length`, `Petal.Length`, `Petal.Width`; як і прогнозувалося раніше, саме за цією компонентою відбувається дискримінація (розрізнення) трьох різних сегментів трьох типів ірисів
-   основне навантаження на другу компоненту складає змінна `Sepal.Width` -- всі три види ірисів можуть мати досить велику варіацію за цим параметром.

```{r}
# Biplot of individuals and variables
fviz_pca_biplot(resPCA,
                geom = c("point"),
                # label = "none", # hide individual labels
             habillage = as.factor(iris$Species), # color by groups
             axes = c(1, 2),
             repel = TRUE,
             label = c("ind", "ind.sup", "quali", "var", "quanti.sup"),
             select.var = list(name = c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")),
             # select.var = list(contrib = 8),
             # label = c("ind.sup"),
             palette = c("#00AFBB", "#E7B800", "#FC4E07", "#00AFBB", "#E7B800", "#FC4E07"),
             # alpha.var = c("contrib"),
             # col.ind = c("contrib"),
             # col.ind.sup = c("contrib"),
             addEllipses = TRUE # Concentration ellipses
             ) +
  theme_minimal()

```

Таким чином, з'ясовано, шо початкові дані не є однорідними. Три типи ірисів різняться за довжинами внутрішніх часток оцвітини (petal length) та шириною внутрішньої частки оцвітини (petal width). Завдяки наявності кореляцій у початкових змінних, спостереження вдалося добре описати у просторі двох інтегральних показників. Знайдені кластери характерні для трьох типів ірисів і у майбутньому можуть бути використані для написання класифікатора з метою розпізнавання нових об'єктів.

### Індивідуальні завдання на лабораторну роботу

Видає викладач.
